{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f342af57",
   "metadata": {},
   "source": [
    "# Legal Contract Clause Classification using Stacked LSTM\n",
    "## CCS 248 – Artificial Neural Networks Final Project\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Automated Classification of Legal Contract Clauses**\n",
    "\n",
    "Lawyers spend hours manually reading and categorizing individual contract clauses (e.g., governing law, termination, confidentiality). This project automates that process using deep learning to classify each clause context into predefined legal categories.\n",
    "\n",
    "## Solution: Stacked Bidirectional LSTM with Attention\n",
    "\n",
    "Using a 2-layer bidirectional LSTM network plus an attention pooling head:\n",
    "- **Bidirectional processing** — reads clauses forward and backward for full context\n",
    "- **Stacked layers + attention** — captures low-level patterns and focuses on salient tokens\n",
    "- **Dropout regularization** — prevents overfitting on legal jargon\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**CUAD v1 master_clauses.csv** (flattened clause snippets)\n",
    "- 1,965 snippets, 40 clause labels originally\n",
    "- Filtered to 7 clause types with at least 5 examples each for stable stratification\n",
    "\n",
    "## Target\n",
    "\n",
    "**Test Accuracy: 50-60%** (course requirement)\n",
    "\n",
    "**Evaluation**: Accuracy, macro F1, per-class precision/recall, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3447d",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac66423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Text processing\n",
    "import string\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# PyTorch for deep learning (avoid Keras)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Scikit-learn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Display versions\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fa1ee",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33840dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clause snippets from CUAD XLSX sheets (label_group_xlsx)\n",
    "import glob\n",
    "XLSX_DIR = r\"D:\\CODE\\ANN FINAL PROJ\\CUAD_v1\\label_group_xlsx\"\n",
    "print(f\"Loading XLSX files from: {XLSX_DIR}\")\n",
    "\n",
    "def load_xlsx_snippets(xlsx_dir: str):\n",
    "    rows = []\n",
    "    files = glob.glob(os.path.join(xlsx_dir, \"*.xlsx\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .xlsx files found in {xlsx_dir}\")\n",
    "    for path in files:\n",
    "        df_x = pd.read_excel(path)\n",
    "        if df_x.empty:\n",
    "            continue\n",
    "        clause_cols = [c for c in df_x.columns if c != df_x.columns[0]]\n",
    "        for _, row in df_x.iterrows():\n",
    "            for col in clause_cols:\n",
    "                text = row[col]\n",
    "                if pd.isna(text):\n",
    "                    continue\n",
    "                text = str(text).strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                rows.append({\"context\": text, \"clause_type\": col})\n",
    "    df_out = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "    return df_out, len(files)\n",
    "\n",
    "df, n_files = load_xlsx_snippets(XLSX_DIR)\n",
    "print(f\"✓ Loaded {len(df)} snippets from {n_files} XLSX files\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset overview\n",
    "print(df.head())\n",
    "print(\"\\nTop clause counts:\")\n",
    "print(df['clause_type'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset stats\n",
    "print(f\"Total snippets: {len(df)}\")\n",
    "print(f\"Unique clause types: {df['clause_type'].nunique()}\")\n",
    "print(f\"Average length (words): {df['context'].apply(lambda x: len(str(x).split())).mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b61748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"First 5 Rows of Dataset:\")\n",
    "print(\"=\"*80)\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*80)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320e091",
   "metadata": {},
   "source": [
    "## 2.1 Data Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clause type distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot of clause counts\n",
    "clause_counts = df['clause_type'].value_counts()\n",
    "axes[0].barh(clause_counts.index[:20], clause_counts.values[:20], color='steelblue')\n",
    "axes[0].set_xlabel('Count', fontsize=12)\n",
    "axes[0].set_ylabel('Clause Type', fontsize=12)\n",
    "axes[0].set_title('Top 20 Clause Types Distribution (Before Filtering)', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Pie chart for proportions\n",
    "top_n = 10\n",
    "top_clauses = clause_counts.head(top_n)\n",
    "other_sum = clause_counts.iloc[top_n:].sum()\n",
    "pie_data = pd.concat([top_clauses, pd.Series({'Other': other_sum})])\n",
    "colors = sns.color_palette('husl', len(pie_data))\n",
    "axes[1].pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axes[1].set_title(f'Top {top_n} Clauses + Other', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal clause types: {len(clause_counts)}\")\n",
    "print(f\"Most common: {clause_counts.index[0]} ({clause_counts.values[0]} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a82741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution analysis\n",
    "df['text_length'] = df['context'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram of text lengths\n",
    "axes[0].hist(df['text_length'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['text_length'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"text_length\"].mean():.1f}')\n",
    "axes[0].axvline(df['text_length'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[\"text_length\"].median():.1f}')\n",
    "axes[0].set_xlabel('Text Length (words)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Clause Text Lengths', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of text lengths\n",
    "sns.boxplot(data=df, y='text_length', ax=axes[1], color='lightblue')\n",
    "axes[1].set_ylabel('Text Length (words)', fontsize=12)\n",
    "axes[1].set_title('Text Length Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"Mean: {df['text_length'].mean():.2f} words\")\n",
    "print(f\"Median: {df['text_length'].median():.2f} words\")\n",
    "print(f\"Min: {df['text_length'].min()} words\")\n",
    "print(f\"Max: {df['text_length'].max()} words\")\n",
    "print(f\"Std: {df['text_length'].std():.2f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ad8e5",
   "metadata": {},
   "source": [
    "# 3. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Top 10 clause types:\")\n",
    "print(df['clause_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66c8cd",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s\\.,;:\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Test\n",
    "sample = \"THIS AGREEMENT is made on January 1, 2020!!!\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After:\", clean_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['context'].apply(clean_text)\n",
    "print(\"✓ Cleaned all documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f08f17",
   "metadata": {},
   "source": [
    "# 5. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cleaned text directly (clause contexts are already short)\n",
    "df['sampled_text'] = df['cleaned_text']\n",
    "print(f\"✓ Using {len(df)} clause contexts (no truncation needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a4d8a",
   "metadata": {},
   "source": [
    "# 6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    \"\"\"Simple tokenizer - built from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_index = {\"<OOV>\": 1}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.word_counts.update(str(text).split())\n",
    "        \n",
    "        most_common = self.word_counts.most_common(self.vocab_size - 2)\n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            self.word_to_index[word] = idx\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word_to_index)}\")\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = [self.word_to_index.get(word, 1) for word in str(text).split()]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "# Tokenizer will be built after filtering to top clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970fb9f",
   "metadata": {},
   "source": [
    "# 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen, padding='post', value=0):\n",
    "    \"\"\"Pad sequences to the same length\"\"\"\n",
    "    padded = np.zeros((len(sequences), maxlen), dtype=np.int32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            if padding == 'post':\n",
    "                padded[i] = seq[:maxlen]\n",
    "            else:\n",
    "                padded[i] = seq[-maxlen:]\n",
    "        else:\n",
    "            if padding == 'post':\n",
    "                padded[i, :len(seq)] = seq\n",
    "            else:\n",
    "                padded[i, -len(seq):] = seq\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63819b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select clause types with enough support to stratify\n",
    "TOP_N = 20\n",
    "MIN_COUNT = 5\n",
    "clause_counts = df['clause_type'].value_counts()\n",
    "filtered_counts = clause_counts[clause_counts >= MIN_COUNT]\n",
    "top_clauses = filtered_counts.head(TOP_N).index.tolist()\n",
    "df_filtered = df[df['clause_type'].isin(top_clauses)].copy()\n",
    "\n",
    "print(f\"Using {len(df_filtered)} samples before augmentation\")\n",
    "print(f\"Top clause types (min {MIN_COUNT} per class):\")\n",
    "for i, (clause, count) in enumerate(filtered_counts.head(TOP_N).items(), 1):\n",
    "    print(f\"  {i}. {clause[:80]}... ({count} samples)\")\n",
    "\n",
    "ENABLE_AUGMENTATION = True\n",
    "TARGET_MIN_PER_CLASS = 30  # desired minimum rows per class after augmentation\n",
    "REPLACE_PROB = 0.25        # probability of replacing a token with a synonym\n",
    "MAX_AUG_PER_CLASS = 80     # cap to avoid explosion per class\n",
    "\n",
    "if ENABLE_AUGMENTATION:\n",
    "    import random\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet as wn\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('omw-1.4', quiet=True)\n",
    "    except Exception as e:\n",
    "        wn = None\n",
    "        print(f\"NLTK/wordnet not available, skipping synonym augmentation: {e}\")\n",
    "\n",
    "    def get_synonyms(word):\n",
    "        if wn is None:\n",
    "            return []\n",
    "        syns = set()\n",
    "        for syn in wn.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                candidate = lemma.name().replace('_', ' ').lower()\n",
    "                if candidate.isalpha() and candidate != word.lower():\n",
    "                    syns.add(candidate)\n",
    "        return list(syns)\n",
    "\n",
    "    def synonym_replace(text, replace_prob=0.2):\n",
    "        tokens = str(text).split()\n",
    "        new_tokens = []\n",
    "        for tok in tokens:\n",
    "            if random.random() < replace_prob:\n",
    "                syns = get_synonyms(tok)\n",
    "                if syns:\n",
    "                    new_tokens.append(random.choice(syns))\n",
    "                    continue\n",
    "            new_tokens.append(tok)\n",
    "        return \" \".join(new_tokens)\n",
    "\n",
    "    aug_rows = []\n",
    "    for label, group in df_filtered.groupby('clause_type'):\n",
    "        current_count = len(group)\n",
    "        if current_count >= TARGET_MIN_PER_CLASS:\n",
    "            continue\n",
    "        needed = min(TARGET_MIN_PER_CLASS - current_count, MAX_AUG_PER_CLASS)\n",
    "        pool = group['sampled_text'].tolist()\n",
    "        for i in range(needed):\n",
    "            base_text = pool[i % len(pool)]\n",
    "            aug_text = synonym_replace(base_text, replace_prob=REPLACE_PROB)\n",
    "            aug_rows.append({\n",
    "                'context': aug_text,\n",
    "                'clause_type': label,\n",
    "                'cleaned_text': aug_text,\n",
    "                'sampled_text': aug_text,\n",
    "            })\n",
    "\n",
    "    if aug_rows:\n",
    "        df_aug = pd.DataFrame(aug_rows)\n",
    "        df_filtered = pd.concat([df_filtered, df_aug], ignore_index=True)\n",
    "        print(f\"Applied augmentation: +{len(aug_rows)} synthetic rows\")\n",
    "    else:\n",
    "        print(\"No augmentation applied (all classes already above target or wordnet unavailable)\")\n",
    "\n",
    "print(f\"Total samples after augmentation: {len(df_filtered)}\")\n",
    "\n",
    "# Build tokenizer on filtered (and possibly augmented) data with smaller vocab to limit noise\n",
    "tokenizer = CustomTokenizer(vocab_size=10000)\n",
    "tokenizer.fit_on_texts(df_filtered['sampled_text'])\n",
    "\n",
    "# Tokenize filtered data\n",
    "sequences_filtered = tokenizer.texts_to_sequences(df_filtered['sampled_text'])\n",
    "\n",
    "# Length stats and padding length\n",
    "sequence_lengths = [len(seq) for seq in sequences_filtered]\n",
    "percentile_len = int(np.percentile(sequence_lengths, 85))\n",
    "MAX_LENGTH = min(percentile_len, 160)\n",
    "print(f\"Sequence length percentile(85th): {percentile_len}\")\n",
    "print(f\"Max sequence length used: {MAX_LENGTH} (capped at 160)\")\n",
    "\n",
    "# Pad filtered sequences\n",
    "X_filtered = pad_sequences(sequences_filtered, maxlen=MAX_LENGTH, padding='post')\n",
    "print(f\"Padded shape (filtered): {X_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce57e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: OOV rate on filtered sequences\n",
    "# OOV token id is 1 in the tokenizer\n",
    "all_tokens = sum(len(seq) for seq in sequences_filtered)\n",
    "oov_tokens = sum(sum(1 for t in seq if t == 1) for seq in sequences_filtered)\n",
    "oov_pct = 100 * oov_tokens / max(1, all_tokens)\n",
    "print(f\"OOV tokens: {oov_tokens} / {all_tokens} ({oov_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels after filtering\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_filtered['clause_type'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Labels shape: {y_encoded.shape}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ac18d",
   "metadata": {},
   "source": [
    "## 7.1 Filtered Dataset Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filtered class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot after filtering\n",
    "filtered_counts = df_filtered['clause_type'].value_counts()\n",
    "colors_bar = sns.color_palette('Set2', len(filtered_counts))\n",
    "axes[0].barh(filtered_counts.index, filtered_counts.values, color=colors_bar)\n",
    "axes[0].set_xlabel('Count', fontsize=12)\n",
    "axes[0].set_ylabel('Clause Type', fontsize=12)\n",
    "axes[0].set_title('Clause Distribution After Filtering & Augmentation', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (idx, val) in enumerate(filtered_counts.items()):\n",
    "    axes[0].text(val + 2, i, str(val), va='center', fontsize=10)\n",
    "\n",
    "# Sequence length distribution\n",
    "seq_lens = [len(seq) for seq in sequences_filtered]\n",
    "axes[1].hist(seq_lens, bins=40, color='teal', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(MAX_LENGTH, color='red', linestyle='--', linewidth=2, label=f'Max Length: {MAX_LENGTH}')\n",
    "axes[1].axvline(np.mean(seq_lens), color='orange', linestyle='--', linewidth=2, label=f'Mean: {np.mean(seq_lens):.1f}')\n",
    "axes[1].set_xlabel('Sequence Length (tokens)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Token Sequence Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFiltered dataset statistics:\")\n",
    "print(f\"Number of classes: {len(filtered_counts)}\")\n",
    "print(f\"Total samples: {len(df_filtered)}\")\n",
    "print(f\"Min samples per class: {filtered_counts.min()}\")\n",
    "print(f\"Max samples per class: {filtered_counts.max()}\")\n",
    "print(f\"Mean samples per class: {filtered_counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Logistic Regression baseline (quick sanity check)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "texts = df_filtered['sampled_text'].astype(str).tolist()\n",
    "labels = y_encoded\n",
    "\n",
    "print('Building TF-IDF matrix...')\n",
    "vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_tfidf = vect.fit_transform(texts)\n",
    "\n",
    "# Split and train a simple linear classifier\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_tfidf, labels, test_size=0.30, random_state=42, stratify=labels)\n",
    "clf = LogisticRegression(max_iter=2000, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_tr, y_tr)\n",
    "acc = clf.score(X_te, y_te)\n",
    "print(f\"TF-IDF Logistic accuracy (test): {acc:.4f}\")\n",
    "\n",
    "# Print detailed per-class report\n",
    "y_pred = clf.predict(X_te)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_te, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da76fde",
   "metadata": {},
   "source": [
    "# 8. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_filtered, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Class weights to handle imbalance (toggle with USE_CLASS_WEIGHTS)\n",
    "class_counts = np.bincount(y_train, minlength=num_classes)\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "class_weights = class_weights * (num_classes / class_weights.sum())\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights (normalized):\", class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "USE_SAMPLER = True\n",
    "\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = ClauseDataset(X_train, y_train)\n",
    "val_dataset = ClauseDataset(X_val, y_val)\n",
    "test_dataset = ClauseDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48211ecb",
   "metadata": {},
   "source": [
    "# 9. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional stacked LSTM with attention for clause classification\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=200, lstm_1=128, lstm_2=96, dropout=0.25, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTM(embed_dim, lstm_1, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(lstm_1 * 2, lstm_2, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.Linear(lstm_2 * 2, 1)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_2 * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        scores = torch.tanh(self.attn(x))\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = (x * weights).sum(dim=1)\n",
    "        context = self.dropout2(context)\n",
    "        return self.fc(context)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_to_index)\n",
    "NUM_CLASSES = num_classes\n",
    "print(f\"Vocab: {VOCAB_SIZE}, Classes: {NUM_CLASSES}, Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a147b",
   "metadata": {},
   "source": [
    "# 10. Hyperparameter Tuning Setup\n",
    "\n",
    "Testing different optimizers as required by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4756ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations to test - tuned for faster convergence with attention\n",
    "configs = [\n",
    "    {'opt': 'Adam',    'lr': 0.0008, 'wd': 1e-4, 'batch': 64, 'epochs': 5},\n",
    "    {'opt': 'Adam',    'lr': 0.0010, 'wd': 1e-4, 'batch': 64, 'epochs': 10},\n",
    "    {'opt': 'Adam',    'lr': 0.0005, 'wd': 1e-4, 'batch': 64, 'epochs': 5},\n",
    "    {'opt': 'RMSprop', 'lr': 0.0008, 'wd': 0.0,  'batch': 64, 'epochs': 10},\n",
    "    {'opt': 'RMSprop', 'lr': 0.0005, 'wd': 0.0,  'batch': 64, 'epochs': 5},\n",
    "]\n",
    "\n",
    "print(f\"Will test {len(configs)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61058ec0",
   "metadata": {},
   "source": [
    "# 11. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run5'\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    model.train() if optimizer else model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        if optimizer:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (preds == y_batch).sum().item()\n",
    "        total_samples += X_batch.size(0)\n",
    "        \n",
    "        # Progress indicator every 50 batches\n",
    "        if optimizer and batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)}\", end='\\r')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def save_model_as_h5(model, filepath):\n",
    "    \"\"\"Save PyTorch model weights to HDF5 format\"\"\"\n",
    "    import h5py\n",
    "    state_dict = model.state_dict()\n",
    "    with h5py.File(filepath, 'w') as f:\n",
    "        for key, value in state_dict.items():\n",
    "            f.create_dataset(key, data=value.cpu().numpy())\n",
    "\n",
    "results = []\n",
    "models_dir = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\trained_models_run5'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "for i, cfg in enumerate(configs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config {i}/{len(configs)}: {cfg['opt']}, LR={cfg['lr']}, WD={cfg['wd']}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "    print(f\"Model created, starting training...\")\n",
    "    \n",
    "    if cfg['opt'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    elif cfg['opt'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('wd', 0.0))\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=0.9, weight_decay=cfg.get('wd', 0.0))\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor if USE_CLASS_WEIGHTS else None)\n",
    "    \n",
    "    if USE_SAMPLER:\n",
    "        sample_weights = class_weights_tensor.cpu().numpy()[y_train]\n",
    "        train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], sampler=train_sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg['batch'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=cfg['batch'], shuffle=False)\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 6\n",
    "    \n",
    "    for epoch in range(cfg['epochs']):\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer=None)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{cfg['epochs']} - Train loss {train_loss:.4f}, acc {train_acc:.4f} | Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Quick val prediction distribution\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_val_preds = []\n",
    "        for Xb, _ in val_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            preds = model(Xb).argmax(dim=1).cpu().numpy()\n",
    "            all_val_preds.extend(preds)\n",
    "    from collections import Counter\n",
    "    pred_dist = Counter(all_val_preds)\n",
    "    print(f\"Val pred distribution: {pred_dist}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = run_epoch(model, test_loader, criterion, optimizer=None)\n",
    "    results.append({\n",
    "        'config': i,\n",
    "        'optimizer': cfg['opt'],\n",
    "        'lr': cfg['lr'],\n",
    "        'wd': cfg.get('wd', 0.0),\n",
    "        'batch_size': cfg['batch'],\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save model in both PyTorch (.pt) and HDF5 (.h5) formats\n",
    "    pt_path = os.path.join(models_dir, f'model_{i}.pt')\n",
    "    h5_path = os.path.join(models_dir, f'model_{i}.h5')\n",
    "    torch.save(model.state_dict(), pt_path)\n",
    "    save_model_as_h5(model, h5_path)\n",
    "    print(f\"Saved: {pt_path} and {h5_path}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0054f0",
   "metadata": {},
   "source": [
    "# 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\experiment_results_run2.csv', index=False)\n",
    "\n",
    "print(\"All Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_idx = results_df['test_acc'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: {best['optimizer']}\")\n",
    "print(f\"Learning Rate: {best['lr']}\")\n",
    "print(f\"Test Accuracy: {best['test_acc']:.2%}\")\n",
    "\n",
    "if best['test_acc'] >= 0.50:\n",
    "    print(\"\\n✓ Meets 50% requirement!\")\n",
    "else:\n",
    "    print(\"\\n✗ Below 50%\")\n",
    "\n",
    "best_model_path = os.path.join(models_dir, f\"model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab18bc",
   "metadata": {},
   "source": [
    "## 12.1 Hyperparameter Tuning Results Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances across different hyperparameters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Train/Val/Test accuracy comparison\n",
    "x_pos = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "axes[0, 0].bar(x_pos - width, results_df['train_acc'], width, label='Train', color='skyblue', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos, results_df['val_acc'], width, label='Val', color='lightgreen', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width, results_df['test_acc'], width, label='Test', color='salmon', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Configuration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_title('Model Accuracy Comparison (Train/Val/Test)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels([f'Config {i+1}' for i in range(len(results_df))], rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].axhline(y=0.50, color='red', linestyle='--', linewidth=2, label='Target: 50%')\n",
    "\n",
    "# 2. Test accuracy by optimizer and learning rate\n",
    "results_df['config_label'] = results_df.apply(lambda r: f\"{r['optimizer']}\\nLR={r['lr']}\", axis=1)\n",
    "colors_opt = ['steelblue' if opt == 'Adam' else 'darkorange' for opt in results_df['optimizer']]\n",
    "axes[0, 1].barh(results_df['config_label'], results_df['test_acc'], color=colors_opt, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Test Accuracy', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Configuration', fontsize=12)\n",
    "axes[0, 1].set_title('Test Accuracy by Optimizer & Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(x=0.50, color='red', linestyle='--', linewidth=2, label='Target: 50%')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Learning rate vs test accuracy scatter\n",
    "for opt in results_df['optimizer'].unique():\n",
    "    opt_data = results_df[results_df['optimizer'] == opt]\n",
    "    axes[1, 0].scatter(opt_data['lr'], opt_data['test_acc'], s=150, alpha=0.7, label=opt)\n",
    "    axes[1, 0].plot(opt_data['lr'], opt_data['test_acc'], alpha=0.5, linestyle='--')\n",
    "axes[1, 0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[1, 0].set_title('Learning Rate vs Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0.50, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# 4. Best model highlight\n",
    "best_config = results_df.loc[results_df['test_acc'].idxmax()]\n",
    "metrics = ['train_acc', 'val_acc', 'test_acc']\n",
    "values = [best_config[m] for m in metrics]\n",
    "colors_metrics = ['skyblue', 'lightgreen', 'salmon']\n",
    "axes[1, 1].bar(metrics, values, color=colors_metrics, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 1].set_title(f'Best Model Performance\\n({best_config[\"optimizer\"]}, LR={best_config[\"lr\"]})', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].axhline(y=0.50, color='red', linestyle='--', linewidth=2, label='Target: 50%')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (metric, val) in enumerate(zip(metrics, values)):\n",
    "    axes[1, 1].text(i, val + 0.02, f'{val:.2%}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest configuration: Config {best_config['config']}\")\n",
    "print(f\"Test Accuracy: {best_config['test_acc']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ff481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact paths for this run\n",
    "ARTIFACTS_DIR = r'd:\\CodingRelated\\Codes.Ams\\ANNFINAL\\artifacts_run5'\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Persist tokenizer and label encoder classes\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'tokenizer_word_index.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer.word_to_index, f)\n",
    "np.save(os.path.join(ARTIFACTS_DIR, 'label_classes.npy'), label_encoder.classes_)\n",
    "\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb130d8",
   "metadata": {},
   "source": [
    "# 13. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model (match training embed_dim)\n",
    "best_model = LSTMClassifier(VOCAB_SIZE, embed_dim=200, num_classes=NUM_CLASSES).to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "# Get predictions\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred = best_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test\n",
    "\n",
    "print(f\"Loaded best model from: model_{best_idx + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix - save and print\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "cm_path = os.path.join(ARTIFACTS_DIR, 'confusion_matrix.csv')\n",
    "cm_df.to_csv(cm_path)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nSaved confusion matrix to: {cm_path}\")\n",
    "print(f\"\\nAccuracy per class:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "    print(f\"{class_name}: {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e0edd",
   "metadata": {},
   "source": [
    "## 13.1 Detailed Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced confusion matrix heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "plt.title('Confusion Matrix - Best Model', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Percentage'},\n",
    "            linewidths=0.5, linecolor='gray',\n",
    "            vmin=0, vmax=1)\n",
    "plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "plt.title('Normalized Confusion Matrix (Row-wise %)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics visualization\n",
    "report_dict = classification_report(y_true_classes, y_pred_classes, \n",
    "                                   target_names=label_encoder.classes_, \n",
    "                                   output_dict=True, zero_division=0)\n",
    "\n",
    "# Extract per-class metrics\n",
    "classes = label_encoder.classes_\n",
    "precision_scores = [report_dict[cls]['precision'] for cls in classes]\n",
    "recall_scores = [report_dict[cls]['recall'] for cls in classes]\n",
    "f1_scores = [report_dict[cls]['f1-score'] for cls in classes]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 1. Precision, Recall, F1 comparison\n",
    "x_pos = np.arange(len(classes))\n",
    "width = 0.25\n",
    "axes[0, 0].bar(x_pos - width, precision_scores, width, label='Precision', color='steelblue', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos, recall_scores, width, label='Recall', color='lightcoral', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width, f1_scores, width, label='F1-Score', color='lightgreen', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Clause Type', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Score', fontsize=12)\n",
    "axes[0, 0].set_title('Per-Class Precision, Recall, and F1-Score', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(classes, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].set_ylim([0, 1.05])\n",
    "\n",
    "# 2. Per-class accuracy from confusion matrix\n",
    "class_accuracies = [cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0 for i in range(len(classes))]\n",
    "colors_acc = ['green' if acc >= 0.5 else 'orange' if acc >= 0.3 else 'red' for acc in class_accuracies]\n",
    "axes[0, 1].barh(classes, class_accuracies, color=colors_acc, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Clause Type', fontsize=12)\n",
    "axes[0, 1].set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Target: 50%')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "for i, (cls, acc) in enumerate(zip(classes, class_accuracies)):\n",
    "    axes[0, 1].text(acc + 0.02, i, f'{acc:.2%}', va='center', fontsize=9)\n",
    "\n",
    "# 3. Support (number of samples per class in test set)\n",
    "support = [report_dict[cls]['support'] for cls in classes]\n",
    "axes[1, 0].bar(classes, support, color='mediumpurple', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Clause Type', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[1, 0].set_title('Test Set Class Distribution (Support)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticklabels(classes, rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (cls, sup) in enumerate(zip(classes, support)):\n",
    "    axes[1, 0].text(i, sup + 0.5, str(sup), ha='center', fontsize=10)\n",
    "\n",
    "# 4. Precision-Recall tradeoff scatter\n",
    "axes[1, 1].scatter(recall_scores, precision_scores, s=200, c=f1_scores, \n",
    "                   cmap='viridis', alpha=0.8, edgecolors='black', linewidth=1.5)\n",
    "for i, cls in enumerate(classes):\n",
    "    axes[1, 1].annotate(cls, (recall_scores[i], precision_scores[i]), \n",
    "                        fontsize=8, ha='center', va='bottom')\n",
    "axes[1, 1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1, 1].set_title('Precision-Recall Scatter (Color = F1-Score)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Diagonal')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "cbar.set_label('F1-Score', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Macro Avg Precision: {report_dict['macro avg']['precision']:.4f}\")\n",
    "print(f\"Macro Avg Recall: {report_dict['macro avg']['recall']:.4f}\")\n",
    "print(f\"Macro Avg F1-Score: {report_dict['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted Avg F1-Score: {report_dict['weighted avg']['f1-score']:.4f}\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_true_classes, y_pred_classes):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee82702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distribution analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# True vs Predicted distribution comparison\n",
    "true_counts = pd.Series(y_true_classes).value_counts().sort_index()\n",
    "pred_counts = pd.Series(y_pred_classes).value_counts().sort_index()\n",
    "\n",
    "# Ensure both have same index\n",
    "all_classes = sorted(set(y_true_classes) | set(y_pred_classes))\n",
    "true_counts = true_counts.reindex(all_classes, fill_value=0)\n",
    "pred_counts = pred_counts.reindex(all_classes, fill_value=0)\n",
    "\n",
    "x_pos = np.arange(len(all_classes))\n",
    "width = 0.35\n",
    "class_names = [label_encoder.classes_[i] for i in all_classes]\n",
    "\n",
    "axes[0].bar(x_pos - width/2, true_counts.values, width, label='True Labels', \n",
    "            color='steelblue', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, pred_counts.values, width, label='Predictions', \n",
    "            color='coral', alpha=0.8)\n",
    "axes[0].set_xlabel('Clause Type', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('True Labels vs Predictions Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Misclassification analysis\n",
    "correct = (y_true_classes == y_pred_classes).sum()\n",
    "incorrect = len(y_true_classes) - correct\n",
    "pie_data = [correct, incorrect]\n",
    "pie_labels = [f'Correct\\n({correct} samples)\\n{correct/len(y_true_classes):.1%}',\n",
    "              f'Misclassified\\n({incorrect} samples)\\n{incorrect/len(y_true_classes):.1%}']\n",
    "colors_pie = ['lightgreen', 'lightcoral']\n",
    "axes[1].pie(pie_data, labels=pie_labels, autopct='%1.1f%%', startangle=90, \n",
    "            colors=colors_pie, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Overall Classification Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClassification Summary:\")\n",
    "print(f\"Total samples: {len(y_true_classes)}\")\n",
    "print(f\"Correctly classified: {correct} ({correct/len(y_true_classes):.2%})\")\n",
    "print(f\"Misclassified: {incorrect} ({incorrect/len(y_true_classes):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5308458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report - save to artifacts\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_true_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_,\n",
    "    output_dict=True,\n",
    "    zero_division=0,\n",
    ")\n",
    "report_df = pd.DataFrame(report).T\n",
    "print(report_df)\n",
    "\n",
    "report_path = os.path.join(ARTIFACTS_DIR, 'classification_report.csv')\n",
    "report_df.to_csv(report_path)\n",
    "print(f\"\\nSaved classification report to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
